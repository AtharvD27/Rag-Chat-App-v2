# ğŸ§  RAG Chat App with Local LLM

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.9+](https://img.shields.io/badge/Python-3.9+-blue.svg)](https://www.python.org/)
[![LangChain](https://img.shields.io/badge/LangChain-supported-success)](https://github.com/langchain-ai/langchain)

A lightweight Retrieval-Augmented Generation (RAG) application that lets you query your own documents (PDF, JSON, HTML, TXT) using a local LLM (e.g., LlamaCpp). This project uses LangChain, ChromaDB, and HuggingFace embeddings.


<!--
---
## ğŸ§© System Architecture

![RAG System Diagram](./docs/rag_system_diagram.png)
<sub><em>Diagram: High-level flow of document loading, vectorstore management, local LLM querying, and session handling.</em></sub>

---
-->

## ğŸš€ Features

- ğŸ“„ Load documents from multiple formats: PDF, JSON, HTML, TXT
- ğŸ§© Automatic chunking & metadata tagging
- ğŸ“¦ Vectorstore with duplicate-checking and persistence (ChromaDB)
- ğŸ¤– Local LLM via LlamaCpp for private & offline QA
- ğŸ’¬ Memory-enabled chat sessions with resume/save capability
- ğŸ›  Configurable and CLI-driven for flexible use

---

## ğŸ§° Tech Stack

- **LangChain** (Chains, Memory, LLM Wrapper)
- **ChromaDB** (Vectorstore)
- **HuggingFace Embeddings**
- **LlamaCpp** (Local LLM inference)
- **Python 3.9+**

---

## ğŸ“ Project Structure

```
â”œâ”€â”€ main.py                  # Entry point for chat app
â”œâ”€â”€ run_vectorstore_update.py  # CLI for vectorstore ops
â”œâ”€â”€ run_chat.py             # Core setup logic
â”œâ”€â”€ chat_agent.py           # RAG chain + source extraction
â”œâ”€â”€ document_loader.py      # Load + split + tag documents
â”œâ”€â”€ vectorstore_manager.py  # Add/check/delete chunks
â”œâ”€â”€ snapshot_manager.py     # Resume/save chat history
â”œâ”€â”€ get_llm.py              # LLM setup (LlamaCpp)
â”œâ”€â”€ utils.py                # Config loading, hashing
â”œâ”€â”€ config.yaml             # App configuration
â””â”€â”€ data/                   # Folder containing your documents
```

---

## ğŸ§ª Setup Instructions

### 1. Clone the repo
```bash
git clone https://github.com/your-username/rag-chat-docs.git
cd rag-chat-docs
```

### 2. Install dependencies
We recommend using a virtual environment.
```bash
pip install -r requirements.txt
```
Make sure your system supports `llama-cpp-python` (install via pip or from source if needed).

### 3. Prepare your local LLM
- Download a `.gguf` model from [Hugging Face](https://huggingface.co/TheBloke)
- Set the path in `config.yaml` under `llm.local_model_path`

Example:
```yaml
llm:
  local_model_path: "/path/to/your/model.gguf"
  temperature: 0.7
  max_tokens: 512
```

### 4. Add your documents
Place your documents inside the `data/` folder.
Supports:
- PDFs
- JSON (with or without `text` field)
- Webpage URLs in `.txt`
- HTML files

---

## ğŸ§  Running the App

### Chat Mode
```bash
python main.py --config config.yaml
```
Interactive CLI allows you to:
- Start/resume sessions
- Ask questions
- Save session history

### Vectorstore Management (Optional)
```bash
# Update with new documents only
python run_vectorstore_update.py --update

# Delete vectorstore
python run_vectorstore_update.py --delete

# Reset vectorstore from scratch
python run_vectorstore_update.py --reset
```

---

## âš™ï¸ Configuration File (`config.yaml`)

```yaml
llm:
  local_model_path: "./models/mistral.gguf"
  temperature: 0.7
  max_tokens: 512

embedding:
  model_name: "all-MiniLM-L6-v2"

chunk:
  size: 800
  overlap: 80

data_path: "./data"
vector_db_path: "./vector_db"
snapshot_path: "./snapshots"
prompt_path: "./prompts.yaml"
```

---

## ğŸ’¾ Saving and Resuming Sessions
- Sessions are stored in `./snapshots`
- Chat history includes questions, answers, and source chunks
- Resume by session ID or alias

---

## ğŸ“ To-Do / Ideas
- [ ] Web interface (Streamlit/FastAPI)
- [ ] Add support for DOCX and XLSX
- [ ] Multi-modal support (images)
- [ ] Dockerization

---

## ğŸ“„ License
MIT License.

---

## ğŸ™Œ Acknowledgments
- [LangChain](https://github.com/langchain-ai/langchain)
- [Chroma](https://www.trychroma.com/)
- [Hugging Face Models](https://huggingface.co/)
- [Llama.cpp](https://github.com/ggerganov/llama.cpp)

---

For issues or enhancements, feel free to open an [issue](https://github.com/your-username/rag-chat-docs/issues).